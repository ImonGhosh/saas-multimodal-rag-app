"""
Document embedding generation for vector search.
"""

import os
import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import json

from openai import RateLimitError, APIError
from dotenv import load_dotenv
from langfuse import Langfuse

from .chunker import DocumentChunk

# Import flexible providers
try:
    from ..utils.providers import get_embedding_client, get_embedding_model
except ImportError:
    # For direct execution or testing
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from utils.providers import get_embedding_client, get_embedding_model

# Load environment variables
load_dotenv()

logger = logging.getLogger(__name__)

# Initialize client with flexible provider
embedding_client = get_embedding_client()
EMBEDDING_MODEL = get_embedding_model()

# Observability (Langfuse) - optional at runtime
try:
    from ..utils.observability import (
        start_span,
        end_span,
        text_payload,
        store_prompts,
        store_responses,
        langfuse_enabled,
        get_current_trace,
    )
except ImportError:
    try:
        from api.utils.observability import (
            start_span,
            end_span,
            text_payload,
            store_prompts,
            store_responses,
            langfuse_enabled,
            get_current_trace,
        )
    except ImportError:
        from utils.observability import (  # type: ignore
            start_span,
            end_span,
            text_payload,
            store_prompts,
            store_responses,
            langfuse_enabled,
            get_current_trace,
        )

_LANGFUSE_CLIENT: Langfuse | None = None


def _get_langfuse_client() -> Langfuse | None:
    global _LANGFUSE_CLIENT
    if _LANGFUSE_CLIENT is not None:
        return _LANGFUSE_CLIENT
    if not langfuse_enabled():
        return None
    try:
        _LANGFUSE_CLIENT = Langfuse()
        return _LANGFUSE_CLIENT
    except Exception as err:
        logger.warning("Failed to initialize Langfuse client for embeddings: %s", err)
        _LANGFUSE_CLIENT = None
        return None


def _start_embedding_generation(
    *,
    name: str,
    model: str,
    input_payload: Dict[str, Any],
    metadata: Dict[str, Any] | None = None,
):
    trace = get_current_trace()
    if trace is not None:
        try:
            return trace.generation(
                name=name,
                model=model,
                input=input_payload,
                metadata=metadata or {},
            )
        except Exception:
            pass

    client = _get_langfuse_client()
    if client is None:
        return None
    try:
        return client.generation(
            name=name,
            model=model,
            input=input_payload,
            metadata=metadata or {},
        )
    except Exception as err:
        logger.debug("Failed to start embedding generation: %s", err)
        return None


class EmbeddingGenerator:
    """Generates embeddings for document chunks."""
    
    def __init__(
        self,
        model: str = EMBEDDING_MODEL,
        batch_size: int = 100,
        max_retries: int = 3,
        retry_delay: float = 1.0
    ):
        """
        Initialize embedding generator.
        
        Args:
            model: OpenAI embedding model to use
            batch_size: Number of texts to process in parallel
            max_retries: Maximum number of retry attempts
            retry_delay: Delay between retries in seconds
        """
        self.model = model
        self.batch_size = batch_size
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        
        # Model-specific configurations
        self.model_configs = {
            "text-embedding-3-small": {"dimensions": 1536, "max_tokens": 8191},
            "text-embedding-3-large": {"dimensions": 3072, "max_tokens": 8191},
            "text-embedding-ada-002": {"dimensions": 1536, "max_tokens": 8191}
        }
        
        if model not in self.model_configs:
            logger.warning(f"Unknown model {model}, using default config")
            self.config = {"dimensions": 1536, "max_tokens": 8191}
        else:
            self.config = self.model_configs[model]
    
    async def generate_embedding(self, text: str) -> List[float]:
        """
        Generate embedding for a single text.
        
        Args:
            text: Text to embed
        
        Returns:
            Embedding vector
        """
        # Truncate text if too long
        if len(text) > self.config["max_tokens"] * 4:  # Rough token estimation
            text = text[:self.config["max_tokens"] * 4]
        
        span = start_span(
            "embed.single",
            input=text_payload(text, store=store_prompts()),
            metadata={"model": self.model},
        )
        generation = _start_embedding_generation(
            name="embed.single",
            model=self.model,
            input_payload=text_payload(text, store=store_prompts()),
            metadata={"model": self.model},
        )
        for attempt in range(self.max_retries):
            try:
                response = await embedding_client.embeddings.create(
                    model=self.model,
                    input=text
                )
                
                embedding = response.data[0].embedding
                usage_details = response.usage.model_dump() if response.usage else None
                if generation is not None:
                    generation.end(
                        output={"embedding_dim": len(embedding)},
                        usage_details=usage_details,
                    )
                end_span(span, metadata={"attempt": attempt + 1, "embedding_dim": len(embedding)})
                return embedding
                
            except RateLimitError as e:
                if attempt == self.max_retries - 1:
                    if generation is not None:
                        generation.end(level="ERROR", status_message=str(e))
                    end_span(span, error=e)
                    raise
                
                # Exponential backoff for rate limits
                delay = self.retry_delay * (2 ** attempt)
                logger.warning(f"Rate limit hit, retrying in {delay}s")
                await asyncio.sleep(delay)
                
            except APIError as e:
                logger.error(f"OpenAI API error: {e}")
                if attempt == self.max_retries - 1:
                    if generation is not None:
                        generation.end(level="ERROR", status_message=str(e))
                    end_span(span, error=e)
                    raise
                await asyncio.sleep(self.retry_delay)
                
            except Exception as e:
                logger.error(f"Unexpected error generating embedding: {e}")
                if attempt == self.max_retries - 1:
                    if generation is not None:
                        generation.end(level="ERROR", status_message=str(e))
                    end_span(span, error=e)
                    raise
                await asyncio.sleep(self.retry_delay)
    
    async def generate_embeddings_batch(
        self,
        texts: List[str]
    ) -> List[List[float]]:
        """
        Generate embeddings for a batch of texts.
        
        Args:
            texts: List of texts to embed
        
        Returns:
            List of embedding vectors
        """
        # Filter and truncate texts
        processed_texts = []
        for text in texts:
            if not text or not text.strip():
                processed_texts.append("")
                continue
                
            # Truncate if too long
            if len(text) > self.config["max_tokens"] * 4:
                text = text[:self.config["max_tokens"] * 4]
            
            processed_texts.append(text)
        
        span = start_span(
            "embed.batch",
            input={"count": len(processed_texts)},
            metadata={"model": self.model},
        )
        generation = _start_embedding_generation(
            name="embed.batch",
            model=self.model,
            input_payload={"count": len(processed_texts)},
            metadata={"model": self.model},
        )
        for attempt in range(self.max_retries):
            try:
                response = await embedding_client.embeddings.create(
                    model=self.model,
                    input=processed_texts
                )
                
                embeddings = [data.embedding for data in response.data]
                usage_details = response.usage.model_dump() if response.usage else None
                if generation is not None:
                    generation.end(
                        output={"count": len(embeddings), "embedding_dim": len(embeddings[0]) if embeddings else 0},
                        usage_details=usage_details,
                    )
                end_span(
                    span,
                    metadata={"attempt": attempt + 1, "count": len(embeddings), "embedding_dim": len(embeddings[0]) if embeddings else 0},
                )
                return embeddings
                
            except RateLimitError as e:
                if attempt == self.max_retries - 1:
                    if generation is not None:
                        generation.end(level="ERROR", status_message=str(e))
                    end_span(span, error=e)
                    raise
                
                delay = self.retry_delay * (2 ** attempt)
                logger.warning(f"Rate limit hit, retrying batch in {delay}s")
                await asyncio.sleep(delay)
                
            except APIError as e:
                logger.error(f"OpenAI API error in batch: {e}")
                if attempt == self.max_retries - 1:
                    if generation is not None:
                        generation.end(level="ERROR", status_message=str(e))
                    end_span(span, error=e)
                    # Fallback to individual processing
                    return await self._process_individually(processed_texts)
                await asyncio.sleep(self.retry_delay)
                
            except Exception as e:
                logger.error(f"Unexpected error in batch embedding: {e}")
                if attempt == self.max_retries - 1:
                    if generation is not None:
                        generation.end(level="ERROR", status_message=str(e))
                    end_span(span, error=e)
                    return await self._process_individually(processed_texts)
                await asyncio.sleep(self.retry_delay)
    
    async def _process_individually(
        self,
        texts: List[str]
    ) -> List[List[float]]:
        """
        Process texts individually as fallback.
        
        Args:
            texts: List of texts to embed
        
        Returns:
            List of embedding vectors
        """
        embeddings = []
        span = start_span(
            "embed.batch_fallback",
            input={"count": len(texts)},
            metadata={"model": self.model},
        )
        
        for text in texts:
            try:
                if not text or not text.strip():
                    embeddings.append([0.0] * self.config["dimensions"])
                    continue
                
                embedding = await self.generate_embedding(text)
                embeddings.append(embedding)
                
                # Small delay to avoid overwhelming the API
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.error(f"Failed to embed text: {e}")
                # Use zero vector as fallback
                embeddings.append([0.0] * self.config["dimensions"])
        
        end_span(span, metadata={"count": len(embeddings)})
        return embeddings
    
    async def embed_chunks(
        self,
        chunks: List[DocumentChunk],
        progress_callback: Optional[callable] = None
    ) -> List[DocumentChunk]:
        """
        Generate embeddings for document chunks.
        
        Args:
            chunks: List of document chunks
            progress_callback: Optional callback for progress updates
        
        Returns:
            Chunks with embeddings added
        """
        if not chunks:
            return chunks
        
        logger.info(f"Generating embeddings for {len(chunks)} chunks")
        span = start_span(
            "embed.chunks",
            input={"chunks": len(chunks), "batch_size": self.batch_size},
            metadata={"model": self.model},
        )
        
        # Process chunks in batches
        embedded_chunks = []
        total_batches = (len(chunks) + self.batch_size - 1) // self.batch_size
        
        for i in range(0, len(chunks), self.batch_size):
            batch_chunks = chunks[i:i + self.batch_size]
            batch_texts = [chunk.content for chunk in batch_chunks]
            
            try:
                # Generate embeddings for this batch
                embeddings = await self.generate_embeddings_batch(batch_texts)
                
                # Add embeddings to chunks
                for chunk, embedding in zip(batch_chunks, embeddings):
                    # Create a new chunk with embedding
                    embedded_chunk = DocumentChunk(
                        content=chunk.content,
                        index=chunk.index,
                        start_char=chunk.start_char,
                        end_char=chunk.end_char,
                        metadata={
                            **chunk.metadata,
                            "embedding_model": self.model,
                            "embedding_generated_at": datetime.now().isoformat()
                        },
                        token_count=chunk.token_count
                    )
                    
                    # Add embedding as a separate attribute
                    embedded_chunk.embedding = embedding
                    embedded_chunks.append(embedded_chunk)
                
                # Progress update
                current_batch = (i // self.batch_size) + 1
                if progress_callback:
                    progress_callback(current_batch, total_batches)
                
                logger.info(f"Processed batch {current_batch}/{total_batches}")
                
            except Exception as e:
                logger.error(f"Failed to process batch {i//self.batch_size + 1}: {e}")
                
                # Add chunks without embeddings as fallback
                for chunk in batch_chunks:
                    chunk.metadata.update({
                        "embedding_error": str(e),
                        "embedding_generated_at": datetime.now().isoformat()
                    })
                    chunk.embedding = [0.0] * self.config["dimensions"]
                    embedded_chunks.append(chunk)
        
        logger.info(f"Generated embeddings for {len(embedded_chunks)} chunks")
        end_span(span, metadata={"embedded_chunks": len(embedded_chunks), "batches": total_batches})
        return embedded_chunks
    
    async def embed_query(self, query: str) -> List[float]:
        """
        Generate embedding for a search query.
        
        Args:
            query: Search query
        
        Returns:
            Query embedding
        """
        span = start_span(
            "embed.query",
            input=text_payload(query, store=store_prompts()),
            metadata={"model": self.model},
        )
        try:
            embedding = await self.generate_embedding(query)
            end_span(span, metadata={"embedding_dim": len(embedding)})
            return embedding
        except Exception as e:
            end_span(span, error=e)
            raise
    
    def get_embedding_dimension(self) -> int:
        """Get the dimension of embeddings for this model."""
        return self.config["dimensions"]


# Cache for embeddings
class EmbeddingCache:
    """Simple in-memory cache for embeddings."""
    
    def __init__(self, max_size: int = 1000):
        """Initialize cache."""
        self.cache: Dict[str, List[float]] = {}
        self.access_times: Dict[str, datetime] = {}
        self.max_size = max_size
    
    def get(self, text: str) -> Optional[List[float]]:
        """Get embedding from cache."""
        text_hash = self._hash_text(text)
        if text_hash in self.cache:
            self.access_times[text_hash] = datetime.now()
            return self.cache[text_hash]
        return None
    
    def put(self, text: str, embedding: List[float]):
        """Store embedding in cache."""
        text_hash = self._hash_text(text)
        
        # Evict oldest entries if cache is full
        if len(self.cache) >= self.max_size:
            oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
            del self.cache[oldest_key]
            del self.access_times[oldest_key]
        
        self.cache[text_hash] = embedding
        self.access_times[text_hash] = datetime.now()
    
    def _hash_text(self, text: str) -> str:
        """Generate hash for text."""
        import hashlib
        return hashlib.md5(text.encode()).hexdigest()


# Factory function
def create_embedder(
    model: str = EMBEDDING_MODEL,
    use_cache: bool = True,
    **kwargs
) -> EmbeddingGenerator:
    """
    Create embedding generator with optional caching.
    
    Args:
        model: Embedding model to use
        use_cache: Whether to use caching
        **kwargs: Additional arguments for EmbeddingGenerator
    
    Returns:
        EmbeddingGenerator instance
    """
    embedder = EmbeddingGenerator(model=model, **kwargs)
    
    if use_cache:
        # Add caching capability
        cache = EmbeddingCache()
        original_generate = embedder.generate_embedding
        
        async def cached_generate(text: str) -> List[float]:
            cached = cache.get(text)
            if cached is not None:
                return cached
            
            embedding = await original_generate(text)
            cache.put(text, embedding)
            return embedding
        
        embedder.generate_embedding = cached_generate
    
    return embedder


# Example usage
async def main():
    """Example usage of the embedder."""
    from .chunker import ChunkingConfig, create_chunker
    
    # Create chunker and embedder
    config = ChunkingConfig(chunk_size=200, use_semantic_splitting=False)
    chunker = create_chunker(config)
    embedder = create_embedder()
    
    sample_text = """
    Google's AI initiatives include advanced language models, computer vision,
    and machine learning research. The company has invested heavily in
    transformer architectures and neural network optimization.
    
    Microsoft's partnership with OpenAI has led to integration of GPT models
    into various products and services, making AI accessible to enterprise
    customers through Azure cloud services.
    """
    
    # Chunk the document
    chunks = chunker.chunk_document(
        content=sample_text,
        title="AI Initiatives",
        source="example.md"
    )
    
    print(f"Created {len(chunks)} chunks")
    
    # Generate embeddings
    def progress_callback(current, total):
        print(f"Processing batch {current}/{total}")
    
    embedded_chunks = await embedder.embed_chunks(chunks, progress_callback)
    
    for i, chunk in enumerate(embedded_chunks):
        print(f"Chunk {i}: {len(chunk.content)} chars, embedding dim: {len(chunk.embedding)}")
    
    # Test query embedding
    query_embedding = await embedder.embed_query("Google AI research")
    print(f"Query embedding dimension: {len(query_embedding)}")


if __name__ == "__main__":
    asyncio.run(main())